{"cells":[{"cell_type":"markdown","source":["## Chapter 24, Advanced Analytics and Machine Learning\nYou can add comments in a Spark notebook. For this load this notebook file into databricks interface, double click on this cell as to edit and see what is on the very first line.  Use the same line for your comments.\n\nUsual markdown formatting can be used as well. Google \"markdown format\" to learn more about it.\n\nHere we are presented with basic steps for Machine Learning.\nMore detailed examples are supposed to be in next chapters.\n\nEverything which differs with a corresponding STDG github repository file is marked with **Note**."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\ndenseVec = Vectors.dense(1.0, 2.0, 3.0)\nsize = 3\nidx = [1, 2] # locations of non-zero elements in vector\nvalues = [2.0, 3.0]\nsparseVec = Vectors.sparse(size, idx, values)\nprint(sparseVec)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(3,[1,2],[2.0,3.0])\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["So a vector `sparseVec` in the previous cell has 3 entries, and at indices 1 and 2 it has float values 2 and 3, correspondingly: (2.0, 3.0, 0). Any other is 0. It is not very sparse. In practice sparse vectors have notably less than 10% of non-zero values but I guess it works as example.\n\n\n#### 1st ML Example\n\n**Note:** I added here a line to calculate the data frame number of rows."],"metadata":{}},{"cell_type":"code","source":["df = spark.read.json(\"/databricks-datasets/definitive-guide/data/simple-ml\")\nprint(df.count()) # this is my line\ndf.orderBy(\"value2\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">110\n+-----+----+------+------------------+\ncolor| lab|value1|            value2|\n+-----+----+------+------------------+\ngreen|good|     1|14.386294994851129|\ngreen| bad|    16|14.386294994851129|\n blue| bad|     8|14.386294994851129|\n blue| bad|     8|14.386294994851129|\n blue| bad|    12|14.386294994851129|\ngreen| bad|    16|14.386294994851129|\ngreen|good|    12|14.386294994851129|\n  red|good|    35|14.386294994851129|\n  red|good|    35|14.386294994851129|\n  red| bad|     2|14.386294994851129|\n  red| bad|    16|14.386294994851129|\n  red| bad|    16|14.386294994851129|\n blue| bad|     8|14.386294994851129|\ngreen|good|     1|14.386294994851129|\ngreen|good|    12|14.386294994851129|\n blue| bad|     8|14.386294994851129|\n  red|good|    35|14.386294994851129|\n blue| bad|    12|14.386294994851129|\n  red| bad|    16|14.386294994851129|\ngreen|good|    12|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["**Note:** The cell below which defines `supervised` object was missing in the corresponding github script and I added it from the book. \n\nHere we see an imitation of R formula object.\nRead this article if you want a reminder for it: https://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf\n\nI would like to remind that in R when we fit a linear regression with R formula then every categorical variable is binarized during a formula object construction. It is done in the following way: for each such variable a set of distinct values is extracted. Afterwards for every distinct value so called \"dummy\" variable is created. It has 1s for records where the value appears and 0s otherwise. Therehefore for a categorical variable with 3 distinct values we get 3 dummy variables. Although one of them is usually dropped. \n\nBy their formula code `lab ~ . +color:value1 + color:value2` authors mean the following formula:\n$$\n\\text{lab}= \\text{color}\\cdot w_1 + \\text{value1}\\cdot w_2 + \\text{value2}\\cdot w_3 + \\text{color}\\cdot \\text{value1}\\cdot w_4 + \n\\text{color}\\cdot \\text{value2}\\cdot w_5\n$$\nThe first 3 summands come from a period in the formula. The period means \"all variables but the one to the left of ~ are included\". Next terms with products of variables are called *variable interactions* in Statistics.\n\nThe R Formula is not mandatory. It is possible in pyspark to provide an outcome (\"lab\" in this case) and features by usual Python means, without R formula: with a vector(may be an array) for the outcome and an array for other variables. In this case we are on our own with binarizing categorical variables and adding variable interactions."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nsupervised = RFormula(formula=\"lab ~ . +color:value1 + color:value2\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["fittedRF = supervised.fit(df)\npreparedDF = fittedRF.transform(df)\npreparedDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+------+------------------+--------------------+-----+\ncolor| lab|value1|            value2|            features|label|\n+-----+----+------+------------------+--------------------+-----+\ngreen|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\ngreen|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\ngreen|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\ngreen| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\ngreen|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\ngreen|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\ngreen|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\ngreen| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n+-----+----+------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["As we see our `lab` variable is binarized as `label`. The rest of variables is transformed and in one column as an array named `features`. Looks like it consists of sparse vectors. We get more actual variables because `color` variable has 3 distinct values, meaning that it and its products yield more variables."],"metadata":{}},{"cell_type":"code","source":["train, test = preparedDF.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["print(lr.explainParams())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">aggregationDepth: suggested depth for treeAggregate (&gt;= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxIter: max number of iterations (&gt;= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (&gt;= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class&apos;s threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (&gt;= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["fittedLR = lr.fit(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["**Note:** The line below was not in a corresponding github script but in the book and I added it here."],"metadata":{}},{"cell_type":"code","source":["fittedLR.transform(train).select(\"label\", \"prediction\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+\nlabel|prediction|\n+-----+----------+\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n+-----+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["But the moment of truth comes when we check our model on a test set. \n\n**Note:** The next line was added by me and it is not in a book or the github script."],"metadata":{}},{"cell_type":"code","source":["fittedLR.transform(test).select(\"label\", \"prediction\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+\nlabel|prediction|\n+-----+----------+\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  1.0|       1.0|\n  1.0|       1.0|\n+-----+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Usually people compute some evaluation metric, like accuracy or confusion table. \n\nRegretfully I do not know Spark evaluation metrics yet. In particular, we are to convert a Spark data frame with predictions into RDD. \n\n**Note:** The cell below is added by me.  I did it for my own peace of mind as a standard step for a model evaluation. It is calculated on one worker because although Python methods may be parallelized on CPUs/GPUs of one node but they are not distributed among workers. Of course native Spark methods are better for big data: they are distributed."],"metadata":{}},{"cell_type":"code","source":["labels_predictions = fittedLR.transform(test).select(\"label\", \"prediction\").toPandas()\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(labels_predictions.iloc[ :,0], labels_predictions.iloc[:, 1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">30</span><span class=\"ansired\">]: </span>array([[ 9,  0],\n       [ 0, 19]])</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["The predictions on the test set are perfect."],"metadata":{}},{"cell_type":"markdown","source":["#### 2nd Example"],"metadata":{}},{"cell_type":"code","source":["train, test = df.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["rForm = RFormula()\nlr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nstages = [rForm, lr]\npipeline = Pipeline().setStages(stages)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\nparams = ParamGridBuilder()\\\n  .addGrid(rForm.formula, [\n    \"lab ~ . + color:value1\",\n    \"lab ~ . + color:value1 + color:value2\"])\\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n  .addGrid(lr.regParam, [0.1, 2.0])\\\n  .build()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\\\n  .setMetricName(\"areaUnderROC\")\\\n  .setRawPredictionCol(\"prediction\")\\\n  .setLabelCol(\"label\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.tuning import TrainValidationSplit\ntvs = TrainValidationSplit()\\\n  .setTrainRatio(0.75)\\\n  .setEstimatorParamMaps(params)\\\n  .setEstimator(pipeline)\\\n  .setEvaluator(evaluator)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["Running the cell below resulted in request to install `MLflow` library for a attached cluster. To install `MLflow` library for a particular cluster go to `clusters`, click on your cluster `Libraries`, then click button `Install New` and in the appeared box choose `PyPI`. Put `MLflow` (no quotes) in `Package` box and hit `Install`. Although it worked anyway for me."],"metadata":{}},{"cell_type":"code","source":["tvsFitted = tvs.fit(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["**Note:** Second line in the cell below was missing from the github script and I added it from the book. I commented it because I do not want to use too much memory."],"metadata":{}},{"cell_type":"code","source":["evaluator.evaluate(tvsFitted.transform(test))\n#tvsFitted.write.overwrite().save(\"temp/ModelLocation\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">38</span><span class=\"ansired\">]: </span>0.9166666666666667</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["Note that every run will produce different results. It happens because splittings (for train/test and validations) were done randomly. The greatest in my experience was about 0.95 and it could be as low as 0.88.\n\nThe interesting moment here is that data are the same as before, which we were able to classify correctly. But we used simple method, without regularization. If you add 0 as one of regularization options you will get 1 as a test evaluation metric."],"metadata":{}}],"metadata":{"name":"Spark-Ch-24-Py-withComments","notebookId":967524416478808},"nbformat":4,"nbformat_minor":0}
