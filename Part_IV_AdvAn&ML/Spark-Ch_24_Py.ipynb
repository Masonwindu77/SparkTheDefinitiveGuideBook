{"cells":[{"cell_type":"markdown","source":["## Chapter 24, Advanced Analytics and Machine Learning\n**Mya's Remark:** You can add comments in a Spark notebook. For this add the same line at the top you see in this cell when you double click on it as your cell first line.\nUsual markdown formatting can be used as well.\n\nHere we are presented with basic steps for Machine Learning.\nMore detailed examples follow in next chapters."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\ndenseVec = Vectors.dense(1.0, 2.0, 3.0)\nsize = 3\nidx = [1, 2] # locations of non-zero elements in vector\nvalues = [2.0, 3.0]\nsparseVec = Vectors.sparse(size, idx, values)\nprint(sparseVec)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(3,[1,2],[2.0,3.0])\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["So a vector `sparseVec` in the previous cell has 3 entries, and at indices 1 and 2 it has float values 2 and 3, correspondingly: (2.0, 3.0, 0). Any other is 0. It is not very sparse, but I guess it works as example.\n\n\n#### 1st ML Example\n\nI added here a line to calculate data frame number of rows."],"metadata":{}},{"cell_type":"code","source":["df = spark.read.json(\"/FileStore/tables/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")\nprint(df.count()) # this is my line\ndf.orderBy(\"value2\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">110\n+-----+----+------+------------------+\ncolor| lab|value1|            value2|\n+-----+----+------+------------------+\ngreen|good|     1|14.386294994851129|\ngreen| bad|    16|14.386294994851129|\n blue| bad|     8|14.386294994851129|\n blue| bad|     8|14.386294994851129|\n blue| bad|    12|14.386294994851129|\ngreen| bad|    16|14.386294994851129|\ngreen|good|    12|14.386294994851129|\n  red|good|    35|14.386294994851129|\n  red|good|    35|14.386294994851129|\n  red| bad|     2|14.386294994851129|\n  red| bad|    16|14.386294994851129|\n  red| bad|    16|14.386294994851129|\n blue| bad|     8|14.386294994851129|\ngreen|good|     1|14.386294994851129|\ngreen|good|    12|14.386294994851129|\n blue| bad|     8|14.386294994851129|\n  red|good|    35|14.386294994851129|\n blue| bad|    12|14.386294994851129|\n  red| bad|    16|14.386294994851129|\ngreen|good|    12|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["**Mya's Remarks:** The cell below which defines `supervised` object was missing in the corresponding github script and I added it from the book. Here authors use imitation of R formula object.\nhttps://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf\n\nI would like to remind that in R when we fit a linear regression every categorical variable is binarized. It is done in the following way: for each such variable a set of distinct values is extracted. Afterwards for every distinct value so called \"dummy\" variable is constructed. It has 1s for records where the value appears and 0s otherwise. \n\nBy their code authors mean the following formula:\n$$\n\\text{lab}= \\text{color}\\cdot x_1 + \\text{value1}\\cdot x_2 + \\text{value2}\\cdot x_3 + \\text{color}\\cdot \\text{value1}\\cdot x_4 + \n\\text{color}\\cdot \\text{value2}\\cdot x_5\n$$\nThe first 3 summands come from a period in the formula. The period means \"all columns but the one to the left of ~ are included\".\n\nIt is possible in pyspark to provide an outcome (\"lab\" in this case) and features by usual Python means, without R formula: with a vector for the outcome and an array for other variables. In this case we are on our own with binarizing categorical variables and adding variable interactions."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nsupervised = RFormula(formula=\"lab ~ . +color:value1 + color:value2\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["fittedRF = supervised.fit(df)\npreparedDF = fittedRF.transform(df)\npreparedDF.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+------+------------------+--------------------+-----+\ncolor| lab|value1|            value2|            features|label|\n+-----+----+------+------------------+--------------------+-----+\ngreen|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\ngreen|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\ngreen|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\ngreen| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\ngreen|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\ngreen|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\ngreen|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\ngreen| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n+-----+----+------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["**Mya's Remark:** As we see our `lab` variable is binarized. The rest of variables are transformed and all put in one colunm as an array."],"metadata":{}},{"cell_type":"code","source":["train, test = preparedDF.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["print(lr.explainParams())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">aggregationDepth: suggested depth for treeAggregate (&gt;= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxIter: max number of iterations (&gt;= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (&gt;= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class&apos;s threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (&gt;= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["fittedLR = lr.fit(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["**Mya's Remark:** The line below was not in a corresponding github script but in the book and I added it here."],"metadata":{}},{"cell_type":"code","source":["fittedLR.transform(train).select(\"label\", \"prediction\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+\nlabel|prediction|\n+-----+----------+\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  1.0|       1.0|\n  1.0|       1.0|\n+-----+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["**Mya's Remark:** But the moment of truth comes when we check our model on a test set. This line was added by me and it is not in a book or the github script."],"metadata":{}},{"cell_type":"code","source":["fittedLR.transform(test).select(\"label\", \"prediction\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+\nlabel|prediction|\n+-----+----------+\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  0.0|       0.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n  1.0|       1.0|\n+-----+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["**Mya's Remark:** Usually people compute some evaluation metric, like accuracy or confusion table. \n\nWe will see more on Spark evaluation metircs in Chapters 26 and 27. In particular, we are to convert the table with predictions into RDD. Here is a list of metrics:\n\nhttps://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.evaluation\n\nThe cell below is added by me.  I did it for my own peace of mind. It is calculated on one node because although Python methods may be parallelized on CPUs/GPUs of one node but they are not distributed among workers. Of course native Spark methods are better because they are distributed."],"metadata":{}},{"cell_type":"code","source":["labels_predictions = fittedLR.transform(test).select(\"label\", \"prediction\").toPandas()\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(labels_predictions.iloc[ :,0], labels_predictions.iloc[:, 1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">11</span><span class=\"ansired\">]: </span>array([[22,  0],\n       [ 0, 15]])</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["The predictions on a test set are perfect."],"metadata":{}},{"cell_type":"markdown","source":["#### 2nd Example"],"metadata":{}},{"cell_type":"code","source":["train, test = df.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["rForm = RFormula()\nlr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nstages = [rForm, lr]\npipeline = Pipeline().setStages(stages)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\nparams = ParamGridBuilder()\\\n  .addGrid(rForm.formula, [\n    \"lab ~ . + color:value1\",\n    \"lab ~ . + color:value1 + color:value2\"])\\\n  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n  .addGrid(lr.regParam, [0.1, 2.0])\\\n  .build()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\\\n  .setMetricName(\"areaUnderROC\")\\\n  .setRawPredictionCol(\"prediction\")\\\n  .setLabelCol(\"label\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.tuning import TrainValidationSplit\ntvs = TrainValidationSplit()\\\n  .setTrainRatio(0.75)\\\n  .setEstimatorParamMaps(params)\\\n  .setEstimator(pipeline)\\\n  .setEvaluator(evaluator)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["**Mya's Remark:** Running the cell below resulted in request to install `MLflow` library for a attached cluster. To install `MLflow` library for a particular cluster go to `clusters`, click on your cluster `Libraries`, then click button `Install New` and in the appeared box choose `PyPI`. Put `MLflow` (no quotes) in `Package` box and hit `Install`. Although it worked anyway for me."],"metadata":{}},{"cell_type":"code","source":["tvsFitted = tvs.fit(train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["**Mya's Remark** Second line in the cell below was missing from the github script and I added it from the book. I commented it because I do not want to use too much memory."],"metadata":{}},{"cell_type":"code","source":["evaluator.evaluate(tvsFitted.transform(test))\n#tvsFitted.write.overwrite().save(\"temp/ModelLocation\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">19</span><span class=\"ansired\">]: </span>0.9230769230769231</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["**Mya's Remark.** Note that every run will produce different results. It happens because splittings (for train/test and validations) were done randomly.\n\nThe interesting moment here is that data are the same as before, which we were able to classify correctly. Note that due to validation each fitting was done only on 0.7\\*0.75 = 0.525 of data, or approximately 58 rows. I guess it's too few."],"metadata":{}}],"metadata":{"name":"Spark-Ch_24_Py","notebookId":967524416478808},"nbformat":4,"nbformat_minor":0}
